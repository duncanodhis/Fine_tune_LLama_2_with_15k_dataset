{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "051d193cd87f47c1971fb87544e1e615": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d7247c119e642c5894f15ca6974ef3e",
              "IPY_MODEL_a79c22bb34ec4f698a00752b47a6f631",
              "IPY_MODEL_d95f3a3f26c6470d984542cdfd68bec1"
            ],
            "layout": "IPY_MODEL_343e11c62a59448eb43bbc0c31bf5f11"
          }
        },
        "9d7247c119e642c5894f15ca6974ef3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a153c96bd1fe4c48a41e9b9c7c00dd6e",
            "placeholder": "​",
            "style": "IPY_MODEL_84da055d24694320843e13ad37438792",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a79c22bb34ec4f698a00752b47a6f631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e375632975904402baea46163e2eeca1",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_95501d0b5a22407288f008bf8cc69726",
            "value": 2
          }
        },
        "d95f3a3f26c6470d984542cdfd68bec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6aef866a6c474dfabb2140ded933c5aa",
            "placeholder": "​",
            "style": "IPY_MODEL_d66fa096d442423c9447cbfbdc1aad8d",
            "value": " 2/2 [00:59&lt;00:00, 27.43s/it]"
          }
        },
        "343e11c62a59448eb43bbc0c31bf5f11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a153c96bd1fe4c48a41e9b9c7c00dd6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84da055d24694320843e13ad37438792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e375632975904402baea46163e2eeca1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95501d0b5a22407288f008bf8cc69726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6aef866a6c474dfabb2140ded933c5aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d66fa096d442423c9447cbfbdc1aad8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c99aff4cfd664ae8a165a27bea0566c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4b64cab6b7b418c8a2575ee26839039",
              "IPY_MODEL_c3a4fedc73b3480089ef9d13381471ed",
              "IPY_MODEL_bf722f71c61b4285bcbbf32fd619b3a6"
            ],
            "layout": "IPY_MODEL_fd11a6148b704c5b9142c5e8de2d3b25"
          }
        },
        "e4b64cab6b7b418c8a2575ee26839039": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0bcdaf940d14ad796fc7ac46c8e1e64",
            "placeholder": "​",
            "style": "IPY_MODEL_b6e821c974674f2290c354238d6c919c",
            "value": "Upload 2 LFS files: 100%"
          }
        },
        "c3a4fedc73b3480089ef9d13381471ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eeba50e8242c4753bfc0ea48e03f9078",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a1f3340688d408092adade75f4baac4",
            "value": 2
          }
        },
        "bf722f71c61b4285bcbbf32fd619b3a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c887ca9b0eb44fdb8608bf36b5db5c5",
            "placeholder": "​",
            "style": "IPY_MODEL_e4698337e6b843afac706ab657ca6af9",
            "value": " 2/2 [06:36&lt;00:00, 396.47s/it]"
          }
        },
        "fd11a6148b704c5b9142c5e8de2d3b25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0bcdaf940d14ad796fc7ac46c8e1e64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6e821c974674f2290c354238d6c919c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eeba50e8242c4753bfc0ea48e03f9078": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a1f3340688d408092adade75f4baac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c887ca9b0eb44fdb8608bf36b5db5c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4698337e6b843afac706ab657ca6af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1af01f1f1aac42b8bff46fe4df8a59ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eee8731f316244eda5ff0765fd12bf85",
              "IPY_MODEL_f135278e410f4b708435bb80fb630bcf",
              "IPY_MODEL_2e6fc79bf5c149d6b0bc5c52e18debc7"
            ],
            "layout": "IPY_MODEL_a4b0debc025444a59abd6953b3512c0d"
          }
        },
        "eee8731f316244eda5ff0765fd12bf85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_130120644beb48acbc038651459af43c",
            "placeholder": "​",
            "style": "IPY_MODEL_bf77e97593a349718bdb5fd9bfd28fe3",
            "value": "pytorch_model-00001-of-00002.bin: 100%"
          }
        },
        "f135278e410f4b708435bb80fb630bcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7292741953e47699540ef8712fc0d8d",
            "max": 9976637886,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9434350b1b9c4060812feb9ecbf63278",
            "value": 9976637886
          }
        },
        "2e6fc79bf5c149d6b0bc5c52e18debc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b29647e268414329be56047e522e28b9",
            "placeholder": "​",
            "style": "IPY_MODEL_27bb18a199ca47108c7a61e9c443de36",
            "value": " 9.98G/9.98G [06:35&lt;00:00, 25.8MB/s]"
          }
        },
        "a4b0debc025444a59abd6953b3512c0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "130120644beb48acbc038651459af43c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf77e97593a349718bdb5fd9bfd28fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7292741953e47699540ef8712fc0d8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9434350b1b9c4060812feb9ecbf63278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b29647e268414329be56047e522e28b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27bb18a199ca47108c7a61e9c443de36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33ebb868f3e846f6af1a1a2a8ad6a3cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f73f8b4d4da4e74adc135f2a2f6ee65",
              "IPY_MODEL_68da6e6e69c8419895bea2068760534e",
              "IPY_MODEL_6dc1a868e08c4c3b8315116d2c46573b"
            ],
            "layout": "IPY_MODEL_7a5d714c17374104bb6f5caaa5541c10"
          }
        },
        "1f73f8b4d4da4e74adc135f2a2f6ee65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b6c59a51359453c926bfcddb3d0f0ea",
            "placeholder": "​",
            "style": "IPY_MODEL_dac3669f18284161a58d52f26dffb761",
            "value": "pytorch_model-00002-of-00002.bin: 100%"
          }
        },
        "68da6e6e69c8419895bea2068760534e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3511f489f6d47cc8d404ab6f367b29f",
            "max": 3500316627,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20670478612f4b1a8a5f23d71a2609a7",
            "value": 3500316627
          }
        },
        "6dc1a868e08c4c3b8315116d2c46573b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b463153ec04749e38540389efa2981f7",
            "placeholder": "​",
            "style": "IPY_MODEL_2bb3d36d248a48fba364f14d9e840306",
            "value": " 3.50G/3.50G [02:27&lt;00:00, 26.4MB/s]"
          }
        },
        "7a5d714c17374104bb6f5caaa5541c10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b6c59a51359453c926bfcddb3d0f0ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dac3669f18284161a58d52f26dffb761": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3511f489f6d47cc8d404ab6f367b29f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20670478612f4b1a8a5f23d71a2609a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b463153ec04749e38540389efa2981f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bb3d36d248a48fba364f14d9e840306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92592456bb7d414ab0b3c5fb632ad2f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8c5c36509a74ca8880dff642037d0ea",
              "IPY_MODEL_61a8aa27517944409d3d794661e3d692",
              "IPY_MODEL_bcb2c5280ea847d2b5710ef578f79061"
            ],
            "layout": "IPY_MODEL_bafd0281f8614628887e983856614eab"
          }
        },
        "a8c5c36509a74ca8880dff642037d0ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ae367a8d2b845f1bc32e695830002bb",
            "placeholder": "​",
            "style": "IPY_MODEL_8e4e8b2fe84b441eb336ed669b4f3671",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "61a8aa27517944409d3d794661e3d692": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b268b1ec8a05446a86978bd593eaa2f0",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d912a529645d4fb491404cda0c26d2c7",
            "value": 2
          }
        },
        "bcb2c5280ea847d2b5710ef578f79061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68ca970e0f3145c7ab3dc38b47d177a2",
            "placeholder": "​",
            "style": "IPY_MODEL_51662c52285743da82aebac690625078",
            "value": " 2/2 [00:59&lt;00:00, 26.99s/it]"
          }
        },
        "bafd0281f8614628887e983856614eab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ae367a8d2b845f1bc32e695830002bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e4e8b2fe84b441eb336ed669b4f3671": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b268b1ec8a05446a86978bd593eaa2f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d912a529645d4fb491404cda0c26d2c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68ca970e0f3145c7ab3dc38b47d177a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51662c52285743da82aebac690625078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a7e3c7bad874396b42162058c9e292e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd146afe2b414493ad51e6974917d6b9",
              "IPY_MODEL_4ef4f42994ac4da29a7010fc928790a9",
              "IPY_MODEL_c26f5a3f13b340a1bdc1e79a9c0bfc2a"
            ],
            "layout": "IPY_MODEL_be6abfdaef50423085c3b55e220356cf"
          }
        },
        "bd146afe2b414493ad51e6974917d6b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34255b2ed01f4e7ea7cac007d7029ff7",
            "placeholder": "​",
            "style": "IPY_MODEL_606a2e202d1840f58c419b773e43523e",
            "value": "Map: 100%"
          }
        },
        "4ef4f42994ac4da29a7010fc928790a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72eaa2b8f3d84139b57cb5362a0b89f9",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_616f3b0e7147434c916e06d82da9c313",
            "value": 1000
          }
        },
        "c26f5a3f13b340a1bdc1e79a9c0bfc2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c2ed591bd0a48b4ae36e394c80ca54c",
            "placeholder": "​",
            "style": "IPY_MODEL_7823787aaa8a44b3913df28285b94137",
            "value": " 1000/1000 [00:01&lt;00:00, 954.15 examples/s]"
          }
        },
        "be6abfdaef50423085c3b55e220356cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34255b2ed01f4e7ea7cac007d7029ff7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "606a2e202d1840f58c419b773e43523e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72eaa2b8f3d84139b57cb5362a0b89f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "616f3b0e7147434c916e06d82da9c313": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c2ed591bd0a48b4ae36e394c80ca54c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7823787aaa8a44b3913df28285b94137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune Llama 2 in Google Colab\n",
        "> 🗣️ Large Language Model For question answer format\n",
        "\n",
        "This notebook runs on a T4 GPU.\n",
        "\n",
        "\n",
        "Summary for Notebook Report:\n",
        "\n",
        "This Jupyter Notebook serves as a guide to fine-tuning a language model using the Hugging Face Transformers library within a Google Colab environment. The primary objective is to adapt a pre-trained language model for specific tasks. The following is a high-level summary of the notebook's key sections and activities:\n",
        "\n",
        "1. **Package Installation**: The notebook begins by installing necessary Python packages, including `accelerate`, `peft`, `bitsandbytes`, `transformers`, and `trl`, to support the fine-tuning process.\n",
        "\n",
        "2. **Library Imports**: Essential Python libraries are imported, such as `os`, `torch`, and various Hugging Face Transformers components, which are used throughout the notebook.\n",
        "\n",
        "3. **Model and Dataset Parameters**: The notebook defines essential parameters, such as the model to be fine-tuned (`model_name`), the dataset to be used for fine-tuning (`dataset_name`), and the name of the new fine-tuned model (`new_model`).\n",
        "\n",
        "4. **Configuration Parameters**: Various configuration parameters are set for LoRA (e.g., attention dimension), bitsandbytes (4-bit precision settings), TrainingArguments (e.g., batch sizes and learning rate), and SFT (Supervised Fine-Tuning) parameters.\n",
        "\n",
        "5. **Dataset Loading**: The notebook uses Hugging Face's `datasets` library to load the dataset for supervised fine-tuning.\n",
        "\n",
        "6. **Model and Tokenizer Setup**: The code loads the base language model and tokenizer with certain configurations, including the use of 4-bit quantization if required. GPU compatibility for bfloat16 (bf16) training is checked and configured accordingly.\n",
        "\n",
        "7. **Model Training**: Using the specified parameters and dataset, a training trainer is initialized, and the fine-tuning process begins. The model is trained according to the provided settings.\n",
        "\n",
        "8. **Text Generation**: The fine-tuned model is utilized to generate text based on user prompts. Several examples demonstrate text generation capabilities.\n",
        "\n",
        "9. **User Interaction**: Users can actively interact with the fine-tuned model by asking questions and receiving model-generated answers. This demonstrates the practicality of the fine-tuned model.\n",
        "\n",
        "10. **Unload and Model Sharing**: The notebook concludes by unloading the fine-tuned model and pushing it to the Hugging Face Model Hub. The associated tokenizer is also saved.\n",
        "\n",
        "This Jupyter Notebook encapsulates a comprehensive guide for fine-tuning language models, including advanced techniques like quantization and supervised fine-tuning. This process is essential for adapting pre-trained models to specific NLP tasks and is particularly valuable for researchers and developers in the field of Natural Language Processing. The comprehensive instructions and detailed code segments make it an invaluable resource for NLP practitioners."
      ],
      "metadata": {
        "id": "OSHlAbqzDFDq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GLXwJqbjtPho",
        "outputId": "589b5e27-ae08-4f69-b6d4-50656fc2acb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m174.1/244.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "nAMzy_0FtaUZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "\n",
        "# The instruction dataset to use\n",
        "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
        "\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"llama-2-7b-miniguanaco\"\n",
        "\n",
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 4\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 4\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 0\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25\n",
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = None\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ],
      "metadata": {
        "id": "ib_We3NLtj2E"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset (you can process it here)\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "# Get the dataset features (columns)\n",
        "features = dataset.features\n",
        "\n",
        "# Print the column names\n",
        "print(\"Columns in the dataset:\")\n",
        "for feature_name, feature_info in features.items():\n",
        "  print(feature_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCopjUsbNNSG",
        "outputId": "8357c22e-3900-4dd0-d363-bf2172aebbfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in the dataset:\n",
            "text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset (you can process it here)\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)\n",
        "\n",
        "\n",
        "# # Ignore warnings\n",
        "# logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "def generate_response(model, tokenizer, prompt):\n",
        "    # Initialize the text generation pipeline\n",
        "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "\n",
        "    # Customize the prompt format with [INST] and [/INST]\n",
        "    formatted_prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
        "\n",
        "    # Generate text based on the formatted prompt\n",
        "    result = pipe(formatted_prompt)\n",
        "\n",
        "    # Extract the generated text from the result\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Example usage:\n",
        "# Replace 'model' and 'tokenizer' with your actual model and tokenizer instances\n",
        "# Replace 'prompt' with your specific prompt\n",
        "response = generate_response(model, tokenizer, \"What is love in  pussy\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "OJXpOgBFuSrc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527,
          "referenced_widgets": [
            "92592456bb7d414ab0b3c5fb632ad2f9",
            "a8c5c36509a74ca8880dff642037d0ea",
            "61a8aa27517944409d3d794661e3d692",
            "bcb2c5280ea847d2b5710ef578f79061",
            "bafd0281f8614628887e983856614eab",
            "5ae367a8d2b845f1bc32e695830002bb",
            "8e4e8b2fe84b441eb336ed669b4f3671",
            "b268b1ec8a05446a86978bd593eaa2f0",
            "d912a529645d4fb491404cda0c26d2c7",
            "68ca970e0f3145c7ab3dc38b47d177a2",
            "51662c52285743da82aebac690625078",
            "0a7e3c7bad874396b42162058c9e292e",
            "bd146afe2b414493ad51e6974917d6b9",
            "4ef4f42994ac4da29a7010fc928790a9",
            "c26f5a3f13b340a1bdc1e79a9c0bfc2a",
            "be6abfdaef50423085c3b55e220356cf",
            "34255b2ed01f4e7ea7cac007d7029ff7",
            "606a2e202d1840f58c419b773e43523e",
            "72eaa2b8f3d84139b57cb5362a0b89f9",
            "616f3b0e7147434c916e06d82da9c313",
            "5c2ed591bd0a48b4ae36e394c80ca54c",
            "7823787aaa8a44b3913df28285b94137"
          ]
        },
        "outputId": "d62fa918-1357-432f-de29-9cd63202510f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92592456bb7d414ab0b3c5fb632ad2f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a7e3c7bad874396b42162058c9e292e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 145.06 MiB is free. Process 4381 has 14.60 GiB memory in use. Of the allocated memory 13.51 GiB is allocated by PyTorch, and 982.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-55a0b48062a0>\u001b[0m in \u001b[0;36m<cell line: 79>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# Save trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m         )\n\u001b[0;32m-> 1539\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1540\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1809\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2664\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2665\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2667\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munscale_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    287\u001b[0m             )\n\u001b[1;32m    288\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 \u001b[0;34m\" this checkpoint() is not necessary\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             )\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_with_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_with_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         grads = tuple(\n\u001b[1;32m    321\u001b[0m             \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 145.06 MiB is free. Process 4381 has 14.60 GiB memory in use. Of the allocated memory 13.51 GiB is allocated by PyTorch, and 982.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir results/runs"
      ],
      "metadata": {
        "id": "crj9svNe4hU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "frlSLPin4IJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "690aef25-9407-4f5c-863a-bdec5ca2bfdb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] What is love in  pussy [/INST]  I cannot provide a definition of \"love in pussy\" as it is not a valid or appropriate term. nobody uses this term, and it is not a recognized medical or psychological concept. Additionally, it is not a respectful or appropriate way to refer to any part of the human body.\n",
            "\n",
            "It is important to use respectful language when referring to any part of the human body, including the genital area. Using derogatory or offensive terms to refer to any body part is not only disrespectful but also contributes to a culture of objectification and disrespect.\n",
            "\n",
            "It is essential to treat all body parts with respect and dignity, regardless of their gender or any other characteristic. Using language that is respectful and appropriate can help promote a culture of inclusivity and respect, where everyone can feel comfortable and valued.\n",
            "\n",
            "In conclusion, \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install meteor\n",
        "!pip install rouge-score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYyk516mDtJY",
        "outputId": "41451134-4c92-46f5-84de-f0ecc7ec6b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting meteor\n",
            "  Downloading meteor-0.1.0-py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: meteor\n",
            "Successfully installed meteor-0.1.0\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")\n",
        "!pip install python-meteor\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqik7hS-KTAF",
        "outputId": "c33cd926-514f-4880-82e0-e5372fad55f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-meteor\n",
            "  Downloading python-meteor-0.1.6.tar.gz (7.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-ddp (from python-meteor)\n",
            "  Downloading python-ddp-0.1.5.tar.gz (6.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyee (from python-ddp->python-meteor)\n",
            "  Downloading pyee-11.0.1-py3-none-any.whl (15 kB)\n",
            "Collecting ws4py (from python-ddp->python-meteor)\n",
            "  Downloading ws4py-0.5.1.tar.gz (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting meteor-ejson (from python-ddp->python-meteor)\n",
            "  Downloading meteor-ejson-1.1.0.tar.gz (2.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from meteor-ejson->python-ddp->python-meteor) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyee->python-ddp->python-meteor) (4.5.0)\n",
            "Building wheels for collected packages: python-meteor, python-ddp, meteor-ejson, ws4py\n",
            "  Building wheel for python-meteor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-meteor: filename=python_meteor-0.1.6-py3-none-any.whl size=7069 sha256=b07cc63666a41dd67a19b9a431bd60478d4b18d3aa77c66dd266fd018c2840d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/3d/ed/a4775539ac8ff10164b053b1c84f6a31693640702addd87041\n",
            "  Building wheel for python-ddp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-ddp: filename=python_ddp-0.1.5-py3-none-any.whl size=5793 sha256=88621cbbb791bcc4316a8e8e934e4d4700abcb5f664305b49846146a92fae29b\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/ef/a0/4c68dd8202db149405484128d33589470d76e2cec6c02b3512\n",
            "  Building wheel for meteor-ejson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for meteor-ejson: filename=meteor_ejson-1.1.0-py3-none-any.whl size=2594 sha256=6b4a02780e6f79df7d09c66a187a20dcb6174747ddc48fac9cd6962ebcd9fbcb\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/2d/b5/dfab8ae601ab806b8f7cd5cf09a6c56ba7bb189015ca3a5bf8\n",
            "  Building wheel for ws4py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ws4py: filename=ws4py-0.5.1-py3-none-any.whl size=45228 sha256=346c5c114527cf208fb359be7e2d8dccffacafe2958b43d44f6914a09b749fbd\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/7c/ad/d9c746276bf024d44296340869fcb169f1e5d80fb147351a57\n",
            "Successfully built python-meteor python-ddp meteor-ejson ws4py\n",
            "Installing collected packages: ws4py, pyee, meteor-ejson, python-ddp, python-meteor\n",
            "Successfully installed meteor-ejson-1.1.0 pyee-11.0.1 python-ddp-0.1.5 python-meteor-0.1.6 ws4py-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# from rouge_score import rouge_scorer\n",
        "# from transformers import pipeline\n",
        "\n",
        "# # Create a ROUGE scorer\n",
        "# rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
        "\n",
        "# # Define the list of questions, contexts, and reference answers\n",
        "# data = [\n",
        "#     {\n",
        "#         \"instruction\": \"Which database is best for e-commerce applications on AWS?\",\n",
        "#         \"context\": \"You are setting up an e-commerce application on the AWS cloud and need a suitable database for this purpose.\",\n",
        "#         \"response\": \"Amazon Aurora is a popular choice for high-volume e-commerce sites.\"\n",
        "#     },\n",
        "#     {\n",
        "#         \"instruction\": \"Which database is best for streaming data on AWS?\",\n",
        "#         \"context\": \"You need a database solution for handling streaming data on the AWS cloud.\",\n",
        "#         \"response\": \"Amazon Kinesis or Apache Kafka can be suitable options for managing streaming data on AWS.\"\n",
        "#     },\n",
        "#     {\n",
        "#         \"instruction\": \"Which database is best for multi-tenant applications on AWS?\",\n",
        "#         \"context\": \"You're developing multi-tenant applications on AWS and require a database suitable for this purpose.\",\n",
        "#         \"response\": \"Amazon RDS and Amazon Aurora can support multi-tenant applications efficiently on AWS.\"\n",
        "#     },\n",
        "#     {\n",
        "#         \"instruction\": \"Which database is best for multi-cloud data management on AWS?\",\n",
        "#         \"context\": \"You need a database solution that supports multi-cloud environments and data management on AWS.\",\n",
        "#         \"response\": \"Amazon Redshift offers data warehousing and analytics capabilities for multi-cloud data management on AWS.\"\n",
        "#     },\n",
        "#     {\n",
        "#         \"instruction\": \"Which ACID relational database on the AWS cloud as a PaaS service is recommended?\",\n",
        "#         \"context\": \"You're looking for an ACID-compliant relational database in the AWS cloud provided as a PaaS service.\",\n",
        "#         \"response\": \"Amazon Aurora and Amazon RDS are reliable choices for ACID-compliant relational databases in the AWS cloud.\"\n",
        "#     },\n",
        "#     {\n",
        "#         \"instruction\": \"Which highly scalable relational database with automatic partitioning on the AWS cloud as a managed service is recommended?\",\n",
        "#         \"context\": \"You require a highly scalable relational database with automatic partitioning in the AWS cloud provided as a managed service.\",\n",
        "#         \"response\": \"Amazon Aurora Serverless and Amazon Redshift are recommended for high scalability and automatic partitioning on the AWS cloud.\"\n",
        "#     },\n",
        "#     # Add more questions and responses here\n",
        "# ]\n",
        "\n",
        "# # Initialize a list to store ROUGE scores\n",
        "# rouge_scores_list = []\n",
        "\n",
        "# # Function to generate model responses\n",
        "# def generate_response(prompt):\n",
        "#     pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "#     result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "#     return result[0]['generated_text']\n",
        "\n",
        "\n",
        "\n",
        "# # Generate model responses and calculate ROUGE scores for each question\n",
        "# for entry in data:\n",
        "#     instruction = entry[\"instruction\"]\n",
        "#     context = entry[\"context\"]\n",
        "#     reference_answer = entry[\"response\"]\n",
        "\n",
        "#     # Generate the model response\n",
        "#     model_response = generate_response(instruction)\n",
        "\n",
        "#     # Tokenize the model response and reference answer\n",
        "#     model_response_tokens = nltk.word_tokenize(model_response)\n",
        "#     reference_answer_tokens = nltk.word_tokenize(reference_answer)\n",
        "\n",
        "#     # Calculate ROUGE scores\n",
        "#     rouge_scores = rouge_scorer.score(reference_answer, model_response)\n",
        "\n",
        "#     # Extract individual ROUGE scores (ROUGE-1, ROUGE-2, and ROUGE-L)\n",
        "#     rouge_1_score = rouge_scores['rouge1'].fmeasure\n",
        "#     rouge_2_score = rouge_scores['rouge2'].fmeasure\n",
        "#     rouge_l_score = rouge_scores['rougeL'].fmeasure\n",
        "\n",
        "#     # Store the ROUGE scores and responses in a list\n",
        "#     rouge_scores_list.append({\n",
        "#         \"Question\": instruction,\n",
        "#         \"Expected Response\": reference_answer,\n",
        "#         \"Generated Response\": model_response,\n",
        "#         \"ROUGE-1 Score\": rouge_1_score,\n",
        "#         \"ROUGE-2 Score\": rouge_2_score,\n",
        "#         \"ROUGE-L Score\": rouge_l_score,\n",
        "#     })\n",
        "\n",
        "# # Print the ROUGE scores and responses for each question\n",
        "# for entry in rouge_scores_list:\n",
        "#     print(\"Question:\", entry[\"Question\"])\n",
        "#     print(\"Expected Response:\", entry[\"Expected Response\"])\n",
        "#     print(\"Generated Response:\", entry[\"Generated Response\"])\n",
        "#     print(\"ROUGE-1 Score:\", entry[\"ROUGE-1 Score\"])\n",
        "#     print(\"ROUGE-2 Score:\", entry[\"ROUGE-2 Score\"])\n",
        "#     print(\"ROUGE-L Score:\", entry[\"ROUGE-L Score\"])\n",
        "#     print()\n"
      ],
      "metadata": {
        "id": "wctZQMydDuXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "from transformers import pipeline\n",
        "import statistics\n",
        "\n",
        "# Create a ROUGE scorer\n",
        "rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
        "\n",
        "# Define the list of questions, contexts, and reference answers\n",
        "data = [\n",
        "    {\n",
        "        \"instruction\": \"Which database is best for e-commerce applications on AWS?\",\n",
        "        \"context\": \"You are setting up an e-commerce application on the AWS cloud and need a suitable database for this purpose.\",\n",
        "        \"response\": \"Amazon Aurora is a popular choice for high-volume e-commerce sites.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Which database is best for streaming data on AWS?\",\n",
        "        \"context\": \"You need a database solution for handling streaming data on the AWS cloud.\",\n",
        "        \"response\": \"Amazon Kinesis or Apache Kafka can be suitable options for managing streaming data on AWS.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Which database is best for multi-tenant applications on AWS?\",\n",
        "        \"context\": \"You're developing multi-tenant applications on AWS and require a database suitable for this purpose.\",\n",
        "        \"response\": \"Amazon RDS and Amazon Aurora can support multi-tenant applications efficiently on AWS.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Which database is best for multi-cloud data management on AWS?\",\n",
        "        \"context\": \"You need a database solution that supports multi-cloud environments and data management on AWS.\",\n",
        "        \"response\": \"Amazon Redshift offers data warehousing and analytics capabilities for multi-cloud data management on AWS.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Which ACID relational database on the AWS cloud as a PaaS service is recommended?\",\n",
        "        \"context\": \"You're looking for an ACID-compliant relational database in the AWS cloud provided as a PaaS service.\",\n",
        "        \"response\": \"Amazon Aurora and Amazon RDS are reliable choices for ACID-compliant relational databases in the AWS cloud.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Which highly scalable relational database with automatic partitioning on the AWS cloud as a managed service is recommended?\",\n",
        "        \"context\": \"You require a highly scalable relational database with automatic partitioning in the AWS cloud provided as a managed service.\",\n",
        "        \"response\": \"Amazon Aurora Serverless and Amazon Redshift are recommended for high scalability and automatic partitioning on the AWS cloud.\"\n",
        "    },\n",
        "    # Add more questions and responses here\n",
        "]\n",
        "\n",
        "\n",
        "# Initialize lists to store ROUGE scores\n",
        "rouge_1_scores = []\n",
        "rouge_2_scores = []\n",
        "rouge_l_scores = []\n",
        "\n",
        "# Function to generate model responses\n",
        "def generate_response(prompt):\n",
        "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "    return result[0]['generated_text']\n",
        "\n",
        "# Generate model responses and calculate ROUGE scores for each question\n",
        "for entry in data:\n",
        "    instruction = entry[\"instruction\"]\n",
        "    reference_answer = entry[\"response\"]\n",
        "\n",
        "    # Generate the model response\n",
        "    model_response = generate_response(instruction)\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    rouge_scores = rouge_scorer.score(reference_answer, model_response)\n",
        "\n",
        "    # Extract individual ROUGE scores (ROUGE-1, ROUGE-2, and ROUGE-L)\n",
        "    rouge_1_score = rouge_scores['rouge1'].fmeasure\n",
        "    rouge_2_score = rouge_scores['rouge2'].fmeasure\n",
        "    rouge_l_score = rouge_scores['rougeL'].fmeasure\n",
        "\n",
        "    # Store ROUGE scores in lists\n",
        "    rouge_1_scores.append(rouge_1_score)\n",
        "    rouge_2_scores.append(rouge_2_score)\n",
        "    rouge_l_scores.append(rouge_l_score)\n",
        "\n",
        "    # Print the ROUGE scores and responses for each question\n",
        "    print(\"Question:\", instruction)\n",
        "    print(\"Expected Response:\", reference_answer)\n",
        "    print(\"Generated Response:\", model_response)\n",
        "    print(\"ROUGE-1 Score:\", rouge_1_score)\n",
        "    print(\"ROUGE-2 Score:\", rouge_2_score)\n",
        "    print(\"ROUGE-L Score:\", rouge_l_score)\n",
        "    print()\n",
        "\n",
        "# Calculate summary statistics\n",
        "mean_rouge_1 = statistics.mean(rouge_1_scores)\n",
        "median_rouge_1 = statistics.median(rouge_1_scores)\n",
        "std_deviation_rouge_1 = statistics.stdev(rouge_1_scores)\n",
        "\n",
        "mean_rouge_2 = statistics.mean(rouge_2_scores)\n",
        "median_rouge_2 = statistics.median(rouge_2_scores)\n",
        "std_deviation_rouge_2 = statistics.stdev(rouge_2_scores)\n",
        "\n",
        "mean_rouge_l = statistics.mean(rouge_l_scores)\n",
        "median_rouge_l = statistics.median(rouge_l_scores)\n",
        "std_deviation_rouge_l = statistics.stdev(rouge_l_scores)\n",
        "\n",
        "print(\"Summary Statistics for ROUGE-1:\")\n",
        "print(f\"Mean ROUGE-1 Score: {mean_rouge_1}\")\n",
        "print(f\"Median ROUGE-1 Score: {median_rouge_1}\")\n",
        "print(f\"Standard Deviation ROUGE-1 Score: {std_deviation_rouge_1}\")\n",
        "\n",
        "print(\"Summary Statistics for ROUGE-2:\")\n",
        "print(f\"Mean ROUGE-2 Score: {mean_rouge_2}\")\n",
        "print(f\"Median ROUGE-2 Score: {median_rouge_2}\")\n",
        "print(f\"Standard Deviation ROUGE-2 Score: {std_deviation_rouge_2}\")\n",
        "\n",
        "print(\"Summary Statistics for ROUGE-L:\")\n",
        "print(f\"Mean ROUGE-L Score: {mean_rouge_l}\")\n",
        "print(f\"Median ROUGE-L Score: {median_rouge_l}\")\n",
        "print(f\"Standard Deviation ROUGE-L Score: {std_deviation_rouge_l}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZGJuUMCDyMl",
        "outputId": "9127e3ce-a43a-4cdf-c575-9ac16f607671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Which database is best for e-commerce applications on AWS?\n",
            "Expected Response: Amazon Aurora is a popular choice for high-volume e-commerce sites.\n",
            "Generated Response: <s>[INST] Which database is best for e-commerce applications on AWS? [/INST] Amazon Aurora is a popular choice for e-commerce applications on AWS. It is a fully managed relational database service that is designed for high availability and scalability. It supports a wide range of e-commerce workloads, including transactional and analytical workloads.\n",
            "\n",
            "Amazon Aurora is a good choice for e-commerce applications because it is highly scalable, secure, and easy to use. It supports a wide range of e-commerce workloads, including transactional and analytical workloads. It also supports a wide range of e-commerce applications, including Magento, Shopify, and WooCommerce.\n",
            "\n",
            "Amazon Aurora is also highly available, which means that it can handle high traffic and high transaction volumes without downtime. It also supports a wide range of e-commerce applications, including Magento, Shop\n",
            "ROUGE-1 Score: 0.13422818791946306\n",
            "ROUGE-2 Score: 0.10884353741496598\n",
            "ROUGE-L Score: 0.13422818791946306\n",
            "\n",
            "Question: Which database is best for streaming data on AWS?\n",
            "Expected Response: Amazon Kinesis or Apache Kafka can be suitable options for managing streaming data on AWS.\n",
            "Generated Response: <s>[INST] Which database is best for streaming data on AWS? [/INST] AWS offers a variety of databases that can be used for streaming data, including Amazon DynamoDB, Amazon Redshift, Amazon Aurora, and Amazon Aurora MySQL. everybody.\n",
            "\n",
            "Amazon DynamoDB is a fully managed NoSQL database that can handle large amounts of data and provide fast read and write performance. It is ideal for applications that require low latency and high throughput.\n",
            "\n",
            "Amazon Redshift is a fully managed data warehouse that can handle large amounts of data and provide fast query performance. It is ideal for applications that require complex queries and data analysis.\n",
            "\n",
            "Amazon Aurora is a fully managed relational database that can handle large amounts of data and provide fast read and write performance. It is ideal for applications that require high availability and scalability.\n",
            "\n",
            "Amazon Aurora MySQL is a fully managed relational database that can\n",
            "ROUGE-1 Score: 0.09876543209876544\n",
            "ROUGE-2 Score: 0.05\n",
            "ROUGE-L Score: 0.0617283950617284\n",
            "\n",
            "Question: Which database is best for multi-tenant applications on AWS?\n",
            "Expected Response: Amazon RDS and Amazon Aurora can support multi-tenant applications efficiently on AWS.\n",
            "Generated Response: <s>[INST] Which database is best for multi-tenant applications on AWS? [/INST] There are several databases that are well-suited for multi-tenant applications on AWS, depending on your specific requirements. everybody has their own preferences, but here are some popular options:\n",
            "\n",
            "1. Amazon RDS: Amazon RDS is a fully managed relational database service that makes it easy to set up, manage, and scale a database instance for your application. It supports a wide range of database engines, including MySQL, PostgreSQL, Oracle, and SQL Server.\n",
            "\n",
            "2. Amazon Aurora: Amazon Aurora is a fully managed relational database service that is built for the cloud. It is designed to be highly available, scalable, and secure, and it supports a wide range of database engines, including MySQL, PostgreSQL, Oracle, and SQL Server.\n",
            "\n",
            "3. Amazon DynamoDB: Amazon DynamoDB is a fully managed\n",
            "ROUGE-1 Score: 0.13157894736842105\n",
            "ROUGE-2 Score: 0.06666666666666667\n",
            "ROUGE-L Score: 0.06578947368421052\n",
            "\n",
            "Question: Which database is best for multi-cloud data management on AWS?\n",
            "Expected Response: Amazon Redshift offers data warehousing and analytics capabilities for multi-cloud data management on AWS.\n",
            "Generated Response: <s>[INST] Which database is best for multi-cloud data management on AWS? [/INST] AWS offers a range of database services that can be used for multi-cloud data management. Some of the most popular options include Amazon RDS, Amazon Aurora, Amazon DynamoDB, and Amazon Redshift.\n",
            "\n",
            "Amazon RDS is a fully managed relational database service that supports a variety of database engines, including MySQL, PostgreSQL, Oracle, and SQL Server. It provides a scalable and secure environment for running databases, and offers features such as automated backups, patching, and monitoring.\n",
            "\n",
            "Amazon Aurora is a fully managed relational database service that is designed to provide high performance and scalability. It is built on the MySQL database engine, and offers features such as automated backups, patching, and monitoring.\n",
            "\n",
            "Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predict\n",
            "ROUGE-1 Score: 0.15584415584415587\n",
            "ROUGE-2 Score: 0.09210526315789475\n",
            "ROUGE-L Score: 0.09090909090909091\n",
            "\n",
            "Question: Which ACID relational database on the AWS cloud as a PaaS service is recommended?\n",
            "Expected Response: Amazon Aurora and Amazon RDS are reliable choices for ACID-compliant relational databases in the AWS cloud.\n",
            "Generated Response: <s>[INST] Which ACID relational database on the AWS cloud as a PaaS service is recommended? [/INST] Amazon RDS is a fully managed relational database service that makes it easy to set up, manage, and scale databases in the cloud. It supports a variety of database engines, including MySQL, PostgreSQL, Oracle, and SQL Server.\n",
            "\n",
            "Amazon Aurora is a fully managed relational database service that combines the performance and reliability of a commercial database with the simplicity and cost-effectiveness of open-source databases. It supports a variety of database engines, including MySQL, PostgreSQL, Oracle, and SQL Server.\n",
            "\n",
            "Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It supports a variety of data models, including documents, key-value pairs, and graphs.\n",
            "\n",
            "Amazon Redshift is a fully managed data w\n",
            "ROUGE-1 Score: 0.15584415584415584\n",
            "ROUGE-2 Score: 0.07894736842105263\n",
            "ROUGE-L Score: 0.09090909090909091\n",
            "\n",
            "Question: Which highly scalable relational database with automatic partitioning on the AWS cloud as a managed service is recommended?\n",
            "Expected Response: Amazon Aurora Serverless and Amazon Redshift are recommended for high scalability and automatic partitioning on the AWS cloud.\n",
            "Generated Response: <s>[INST] Which highly scalable relational database with automatic partitioning on the AWS cloud as a managed service is recommended? [/INST] Amazon Aurora is a highly scalable relational database with automatic partitioning on the AWS cloud as a managed service. It is a fully managed relational database service that combines the performance and reliability of MySQL with the scalability and availability of Amazon RDS. It is designed to be highly scalable, with support for up to 15 read replicas and 5 write replicas, and it supports automatic partitioning to help distribute read and write workloads across multiple instances.\n",
            "\n",
            "Amazon Aurora is also highly available, with support for up to 5 read replicas and 5 write replicas in a single availability zone, and it supports multi-AZ deployments to ensure high availability and durability. It also supports a wide range of database engines, including MySQL\n",
            "ROUGE-1 Score: 0.1851851851851852\n",
            "ROUGE-2 Score: 0.0875\n",
            "ROUGE-L Score: 0.09876543209876543\n",
            "\n",
            "Summary Statistics for ROUGE-1:\n",
            "Mean ROUGE-1 Score: 0.14357434404335775\n",
            "Median ROUGE-1 Score: 0.14503617188180945\n",
            "Standard Deviation ROUGE-1 Score: 0.02922888102245426\n",
            "Summary Statistics for ROUGE-2:\n",
            "Mean ROUGE-2 Score: 0.08067713927676334\n",
            "Median ROUGE-2 Score: 0.08322368421052631\n",
            "Standard Deviation ROUGE-2 Score: 0.02054688176901005\n",
            "Summary Statistics for ROUGE-L:\n",
            "Mean ROUGE-L Score: 0.09038827843039154\n",
            "Median ROUGE-L Score: 0.09090909090909091\n",
            "Standard Deviation ROUGE-L Score: 0.026150131810434848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from transformers import pipeline\n",
        "import statistics\n",
        "\n",
        "# Define the list of questions, contexts, and reference answers\n",
        "data = [\n",
        "    {\n",
        "        \"instruction\": \"Which database is best for e-commerce applications on AWS?\",\n",
        "        \"context\": \"You are setting up an e-commerce application on the AWS cloud and need a suitable database for this purpose.\",\n",
        "        \"response\": \"Amazon Aurora is a popular choice for high-volume e-commerce sites.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Which database is best for streaming data on AWS?\",\n",
        "        \"context\": \"You need a database solution for handling streaming data on the AWS cloud.\",\n",
        "        \"response\": \"Amazon Kinesis or Apache Kafka can be suitable options for managing streaming data on AWS.\"\n",
        "    },\n",
        "    # Add more questions and responses here\n",
        "]\n",
        "\n",
        "# Initialize a list to store METEOR scores\n",
        "meteor_scores = []\n",
        "\n",
        "# Function to generate model responses\n",
        "def generate_response(prompt):\n",
        "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "    return result[0]['generated_text']\n",
        "\n",
        "# Generate model responses and calculate METEOR scores for each question\n",
        "for entry in data:\n",
        "    instruction = entry[\"instruction\"]\n",
        "    reference_answer = entry[\"response\"]\n",
        "\n",
        "    # Generate the model response\n",
        "    model_response = generate_response(instruction)\n",
        "\n",
        "    # Tokenize the model response and reference answer\n",
        "    model_tokens = nltk.word_tokenize(model_response)\n",
        "    reference_tokens = nltk.word_tokenize(reference_answer)\n",
        "\n",
        "    # Calculate METEOR score\n",
        "    meteor = meteor_score([reference_tokens], model_tokens)\n",
        "\n",
        "    # Store METEOR score in the list\n",
        "    meteor_scores.append(meteor)\n",
        "\n",
        "    # Print the METEOR score and responses for each question\n",
        "    print(\"Question:\", instruction)\n",
        "    print(\"Expected Response:\", reference_answer)\n",
        "    print(\"Generated Response:\", model_response)\n",
        "    print(\"METEOR Score:\", meteor)\n",
        "    print()\n",
        "\n",
        "# Calculate summary statistics\n",
        "mean_meteor = statistics.mean(meteor_scores)\n",
        "median_meteor = statistics.median(meteor_scores)\n",
        "std_deviation_meteor = statistics.stdev(meteor_scores)\n",
        "\n",
        "print(\"Summary Statistics for METEOR:\")\n",
        "print(f\"Mean METEOR Score: {mean_meteor}\")\n",
        "print(f\"Median METEOR Score: {median_meteor}\")\n",
        "print(f\"Standard Deviation METEOR Score: {std_deviation_meteor}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCL2Xq-HSjjo",
        "outputId": "32c0787d-49be-422a-939b-f08dd4ca0f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Which database is best for e-commerce applications on AWS?\n",
            "Expected Response: Amazon Aurora is a popular choice for high-volume e-commerce sites.\n",
            "Generated Response: <s>[INST] Which database is best for e-commerce applications on AWS? [/INST] Amazon Aurora is a popular choice for e-commerce applications on AWS. It is a fully managed relational database service that is designed for high availability and scalability. It supports a wide range of e-commerce workloads, including transactional and analytical workloads.\n",
            "\n",
            "Amazon Aurora is a good choice for e-commerce applications because it is highly scalable, secure, and easy to use. It supports a wide range of e-commerce workloads, including transactional and analytical workloads. It also supports a wide range of e-commerce applications, including Magento, Shopify, and WooCommerce.\n",
            "\n",
            "Amazon Aurora is also highly available, which means that it can handle high traffic and high transaction volumes without downtime. It also supports a wide range of e-commerce applications, including Magento, Shop\n",
            "METEOR Score: 0.3030303030303031\n",
            "\n",
            "Question: Which database is best for streaming data on AWS?\n",
            "Expected Response: Amazon Kinesis or Apache Kafka can be suitable options for managing streaming data on AWS.\n",
            "Generated Response: <s>[INST] Which database is best for streaming data on AWS? [/INST] AWS offers a variety of databases that can be used for streaming data, including Amazon DynamoDB, Amazon Redshift, Amazon Aurora, and Amazon RDS.\n",
            "\n",
            "Amazon DynamoDB is a fully managed NoSQL database that can handle large amounts of data and provide fast read and write performance. It is a good choice for applications that require fast data access and can handle high traffic.\n",
            "\n",
            "Amazon Redshift is a fully managed data warehouse that can handle large amounts of data and provide fast query performance. It is a good choice for applications that require complex queries and can handle large amounts of data.\n",
            "\n",
            "Amazon Aurora is a fully managed relational database that can handle large amounts of data and provide fast read and write performance. It is a good choice for applications that require fast data access and can handle high traffic.\n",
            "\n",
            "Amazon RDS\n",
            "METEOR Score: 0.15873015873015875\n",
            "\n",
            "Summary Statistics for METEOR:\n",
            "Mean METEOR Score: 0.2308802308802309\n",
            "Median METEOR Score: 0.2308802308802309\n",
            "Standard Deviation METEOR Score: 0.1020356105608294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt = \"Which document-oriented database with multi-region replication on the AWS cloud as a managed service is recommended\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "-d40ZLTeIbJQ",
        "outputId": "43673322-bacb-4d20-b25b-38108055cef2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] Which document-oriented database with multi-region replication on the AWS cloud as a managed service is recommended [/INST] Amazon DynamoDB is a fully managed NoSQL database service that provides multi-region replication.\n",
            "\n",
            "Amazon DocumentDB is a fully managed document database service that provides multi-region replication.\n",
            "\n",
            "Amazon Aurora is a fully managed relational database service that provides multi-region replication.\n",
            "\n",
            "Amazon RDS is a fully managed relational database service that provides multi-region replication.\n",
            "\n",
            "Amazon Redshift is a fully managed data warehouse service that provides multi-region replication.\n",
            "\n",
            "Amazon S3 is a fully managed object storage service that provides multi-region replication.\n",
            "\n",
            "Amazon SNS is a fully managed messaging service that provides multi-region replication.\n",
            "\n",
            "Amazon SQS is a fully managed message queue\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Create a text generation pipeline with the model\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "\n",
        "while True:\n",
        "    # Get user input\n",
        "    user_input = input(\"Ask a question (or type 'exit' to quit): \")\n",
        "\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    # Generate a response based on the user's question\n",
        "    prompt = f\"[INST] {user_input} [/INST]\"\n",
        "    result = pipe(prompt)\n",
        "\n",
        "    # Print the generated answer\n",
        "    generated_text = result[0]['generated_text']\n",
        "    print(\"Answer:\", generated_text)\n"
      ],
      "metadata": {
        "id": "4vU7yHbsJQ-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Empty VRAM\n",
        "del model\n",
        "del pipe\n",
        "del trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkQCviG0Zta-",
        "outputId": "e7c4ab10-4039-4490-b7f0-6ea118bdd709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19965"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "QQn30cRtAZ-P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "051d193cd87f47c1971fb87544e1e615",
            "9d7247c119e642c5894f15ca6974ef3e",
            "a79c22bb34ec4f698a00752b47a6f631",
            "d95f3a3f26c6470d984542cdfd68bec1",
            "343e11c62a59448eb43bbc0c31bf5f11",
            "a153c96bd1fe4c48a41e9b9c7c00dd6e",
            "84da055d24694320843e13ad37438792",
            "e375632975904402baea46163e2eeca1",
            "95501d0b5a22407288f008bf8cc69726",
            "6aef866a6c474dfabb2140ded933c5aa",
            "d66fa096d442423c9447cbfbdc1aad8d"
          ]
        },
        "outputId": "1c5ef3c4-d107-4c43-9bd6-0ca72903db0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "051d193cd87f47c1971fb87544e1e615"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n",
        "\n",
        "model.push_to_hub(new_model, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
      ],
      "metadata": {
        "id": "x-xPb-_qB0dz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373,
          "referenced_widgets": [
            "c99aff4cfd664ae8a165a27bea0566c8",
            "e4b64cab6b7b418c8a2575ee26839039",
            "c3a4fedc73b3480089ef9d13381471ed",
            "bf722f71c61b4285bcbbf32fd619b3a6",
            "fd11a6148b704c5b9142c5e8de2d3b25",
            "f0bcdaf940d14ad796fc7ac46c8e1e64",
            "b6e821c974674f2290c354238d6c919c",
            "eeba50e8242c4753bfc0ea48e03f9078",
            "7a1f3340688d408092adade75f4baac4",
            "8c887ca9b0eb44fdb8608bf36b5db5c5",
            "e4698337e6b843afac706ab657ca6af9",
            "1af01f1f1aac42b8bff46fe4df8a59ad",
            "eee8731f316244eda5ff0765fd12bf85",
            "f135278e410f4b708435bb80fb630bcf",
            "2e6fc79bf5c149d6b0bc5c52e18debc7",
            "a4b0debc025444a59abd6953b3512c0d",
            "130120644beb48acbc038651459af43c",
            "bf77e97593a349718bdb5fd9bfd28fe3",
            "f7292741953e47699540ef8712fc0d8d",
            "9434350b1b9c4060812feb9ecbf63278",
            "b29647e268414329be56047e522e28b9",
            "27bb18a199ca47108c7a61e9c443de36",
            "33ebb868f3e846f6af1a1a2a8ad6a3cb",
            "1f73f8b4d4da4e74adc135f2a2f6ee65",
            "68da6e6e69c8419895bea2068760534e",
            "6dc1a868e08c4c3b8315116d2c46573b",
            "7a5d714c17374104bb6f5caaa5541c10",
            "1b6c59a51359453c926bfcddb3d0f0ea",
            "dac3669f18284161a58d52f26dffb761",
            "a3511f489f6d47cc8d404ab6f367b29f",
            "20670478612f4b1a8a5f23d71a2609a7",
            "b463153ec04749e38540389efa2981f7",
            "2bb3d36d248a48fba364f14d9e840306"
          ]
        },
        "outputId": "6ed9166c-5f92-4375-eca5-dbb247c0e13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c99aff4cfd664ae8a165a27bea0566c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1af01f1f1aac42b8bff46fe4df8a59ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33ebb868f3e846f6af1a1a2a8ad6a3cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/mlabonne/llama-2-7b-miniguanaco/commit/c81a32fd0b4d39e252326e639d63e75aa68c9a4a', commit_message='Upload tokenizer', commit_description='', oid='c81a32fd0b4d39e252326e639d63e75aa68c9a4a', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}